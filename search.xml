<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>KEGG数据库下载</title>
    <url>/2021/01/22/KEGG%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%8B%E8%BD%BD/</url>
    <content><![CDATA[<h3 id="kegg-数据库">1.KEGG 数据库</h3>
<p>KEGG是现在主流的基因注释的数据库之一,KEGG 是一个集成的数据库资源， 主要有18个子库构成，分成四个主要部分，包括： 系统信息（KEGG 代谢通路，简介，模块）； 基因组信息（直系的功能基因，基因组，基因，基因序列相似性）； 化学信息（小分子，聚糖，反应，反应类型，酶命名） 健康信息（疾病和变异相关的网络，人类基因组变异，人类疾病，药物，药物群，健康相关物质）</p>
<h3 id="生物信息学常用的子库介绍">2.生物信息学常用的子库介绍</h3>
<h4 id="kegg-orthology">2.1 KEGG Orthology</h4>
<p>KEGG Orthology(KO)是用功能直系同源基因表示分子功能的数据库。 功能直系同源是手动</p>
]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch实操</title>
    <url>/2021/01/20/Pytorch%E5%AE%9E%E6%93%8D/</url>
    <content><![CDATA[<p>pytorch，深度学习流行的框架之一，Facebook开发维护，社区活跃度高。</p>
<h3 id="pytorch维度转化相关">1.pytorch维度转化相关</h3>
<h4 id="permute-vs-transpose">1.1 permute vs transpose</h4>
<p>词义是重新排列，改变次序的意思，在pytorch中主要用来实现tensor的维度转换。</p>
<p>pytorch官方的解释如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ permute(*dims) → Tensor</span><br><span class="line">$     Returns a view of the original tensor with its dimensions permuted.</span><br><span class="line">$     Parameters</span><br><span class="line">$         *dims (int...) – The desired ordering of dimensions</span><br></pre></td></tr></table></figure>
<p>Example <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.size()</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.permute(<span class="number">3</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>).size()</span><br><span class="line">torch.Size([<span class="number">5</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure> Tensor.permute(a,b,c,d, ...)：permute函数可以对任意高维矩阵进行转置； 但没有 torch.permute() 这个调用方式，只能 Tensor.permute()；</p>
<p>torch.transpose(Tensor, a,b)：transpose只能操作2D矩阵的转置， 但是多次的2D转换的结果和permute的一致。 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.randn(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>).transpose(<span class="number">2</span>,<span class="number">0</span>).shape</span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">5</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.randn(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>).transpose(<span class="number">2</span>,<span class="number">0</span>).transpose(<span class="number">3</span>,<span class="number">0</span>)</span><br><span class="line">torch.Size([<span class="number">5</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.randn(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>).transpose(<span class="number">2</span>,<span class="number">0</span>).transpose(<span class="number">3</span>,<span class="number">0</span>).transpose(<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line">torch.Size([<span class="number">5</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure> 总结，复杂的转换可以使用permute，简答的2D转换用transpose。</p>
<h4 id="view">1.2 view</h4>
<p>改变tensor的形状,但是和permute和transpose不同； 参数中的-1就代表这个位置由其他位置的数字来推断；</p>
<p>pytorch的官方解释如下： Returns a new tensor with the same data as the self tensor but of a different shape.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ view(*shape) → Tensor</span><br><span class="line">$ Parameters</span><br><span class="line">$     shape (torch.Size or int...) – the desired size</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.size()</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">4</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = x.view(<span class="number">16</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y.size()</span><br><span class="line">torch.Size([<span class="number">16</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z = x.view(-<span class="number">1</span>, <span class="number">8</span>)  <span class="comment"># the size -1 is inferred from other dimensions</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z.size()</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">8</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.size()</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = a.transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># Swaps 2nd and 3rd dimension</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b.size()</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c = a.view(<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>)  <span class="comment"># Does not change tensor layout in memory</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c.size()</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.equal(b, c)</span><br><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>tensor</tag>
      </tags>
  </entry>
  <entry>
    <title>VGG解析和实现</title>
    <url>/2021/01/20/VGG%E8%A7%A3%E6%9E%90%E5%92%8C%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<h3 id="vgg16">1. VGG16</h3>
<p>VGG16是运用高深度的卷积神经网络在图像识别方面的一个重要的应用，并在2014年夺得ImageNet的定位第一和分类第二。 <a href="https://arxiv.org/pdf/1409.1556.pdf">论文PDF</a></p>
<h3 id="网络结构">2. 网络结构</h3>
<p>文章中网络结构如下： <img src="/2021/01/20/VGG%E8%A7%A3%E6%9E%90%E5%92%8C%E5%AE%9E%E7%8E%B0/vgg.png" alt="&#39;vgg&#39;"></p>
<h3 id="代码实现">3. 代码实现</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">def conv_layer(chann_in, chann_out, k_size, p_size):</span><br><span class="line">    layer = tnn.Sequential(</span><br><span class="line">        tnn.Conv2d(chann_in, chann_out, kernel_size=k_size, padding=p_size),</span><br><span class="line">        tnn.BatchNorm2d(chann_out),</span><br><span class="line">        tnn.ReLU()</span><br><span class="line">    )</span><br><span class="line">    <span class="built_in">return</span> layer</span><br><span class="line"></span><br><span class="line">def vgg_conv_block(in_list, out_list, k_list, p_list, pooling_k, pooling_s):</span><br><span class="line"></span><br><span class="line">    layers = [ conv_layer(in_list[i], out_list[i], k_list[i], p_list[i]) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(in_list)) ]</span><br><span class="line">    layers += [ tnn.MaxPool2d(kernel_size = pooling_k, stride = pooling_s)]</span><br><span class="line">    <span class="built_in">return</span> tnn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">def vgg_fc_layer(size_in, size_out):</span><br><span class="line">    layer = tnn.Sequential(</span><br><span class="line">        tnn.Linear(size_in, size_out),</span><br><span class="line">        tnn.BatchNorm1d(size_out),</span><br><span class="line">        tnn.ReLU()</span><br><span class="line">    )</span><br><span class="line">    <span class="built_in">return</span> layer</span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>bernoulli_distribution</title>
    <url>/2021/01/25/bernoulli-distribution/</url>
    <content><![CDATA[<h3 id="伯努利分布bernoulli-distribution">1.伯努利分布（bernoulli distribution）</h3>
<p>伯努利分布，又名两点分布或者0-1分布，是一个离散型的随机分布。 如果伯努利试验成功，则伯努利随机变量取值为1，如果失败，伯努利 随机变量取值为0。记其成功概率为p，失败概率为q=1-p.</p>
<p>则其概率质量函数为：</p>
<p><span class="math display">\[f(x) = p^x(1-p)^{1-x} = \begin{cases}
p &amp; if x = 1, \\
q &amp; if x = 0.
\end{cases}\]</span></p>
<h3 id="伯努利分布的期望">2.伯努利分布的期望</h3>
<p><span class="math display">\[E[X] = \sum_{i=0}^{1}x_if(x_i)=1 \cdot p+0 \cdot p=p\]</span></p>
<h3 id="伯努利分布的方差">3.伯努利分布的方差</h3>
<p><span class="math display">\[var[X] = \sum_{i=0}^{1}(x_i-E[X])^2f(x_i) = (0-p)^2(1-p)+(1-p)^2p=p(1-p)=pq\]</span></p>
<p>当<span class="math inline">\(p=q=\frac{1}{2}\)</span>时，方差最大。</p>
]]></content>
      <categories>
        <category>probability</category>
        <category>bernoulli</category>
      </categories>
      <tags>
        <tag>bernoulli distribution</tag>
      </tags>
  </entry>
  <entry>
    <title>binomial_distribution</title>
    <url>/2021/01/25/binomial-distribution/</url>
    <content><![CDATA[<h3 id="二项分布binomial-distribution">1.二项分布(binomial distribution)</h3>
<p>两点分布重复n次，就得到了二项分布，二项分布的概率质量函数(probability mass function, PMF): n为实验的总次数，k为实验成功的次数，p是成功的概率 <span class="math display">\[ P(X=k)=C_n^kp^k(1-p)^{n-k} \]</span> 服从二项分布的随机变量记为 <span class="math inline">\(X \sim B(n,p)\)</span></p>
<h3 id="二项分布的期望">2.二项分布的期望</h3>
<p>二项分布分布，事件发生的概率为p, 不发生的概率为q=1-p, 这里的 <span class="math inline">\(C_n^k\)</span> 称为二项系数，根据二项展开式的系数，可以反推二项分布的概率和为1. <span class="math display">\[ \sum_{k=0}^nP(X=k) = \sum_{k=0}^nC_n^kp^k(1-p)^{n-k} = (p+(1-p))^n=1 \]</span> 期望是离散型随机变量的特征之一，定义如下： 设<span class="math inline">\(\xi\)</span> 为离散型随机变量，它可以取值<span class="math inline">\(x_1,x_2,x_3,...\)</span>，对应的概率为<span class="math inline">\(p_1,p_2,p_3,...\)</span> 如果级数 <span class="math display">\[\sum_{i=1}^{\infty}x_ip_i\]</span> 绝对收敛，则把它称为<span class="math inline">\(\xi\)</span>的数学期望（mathematical expectation）,简称期望，期望值或均值（mean）,记为<span class="math inline">\(E\xi\)</span> 当<span class="math inline">\(\sum_{i=1}^{\infty}{\vert}x_i{\vert}p_i\)</span> 发散时，则<span class="math inline">\(\xi\)</span>的数学期望不存在。</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\sum_{k=0}^{n}kp_k &amp;= \sum_{k=1}{n}{n \choose k}p^kq^{n-k} \\
&amp;= np \sum_{k=1}^{n}{n-1 \choose k-1}p^{k-1}q^{n-k} \\
&amp;=np(p+q)^{n-1} \\
&amp;=np
\end{aligned}
\end{equation}\]</span></p>
<p>二项分布期望证明二:</p>
<p>设<span class="math inline">\(\xi_1,\xi_2,...\xi_n\)</span> 是n个伯努利随机变量，以概率<span class="math inline">\(P\{\xi_i=1\} = p\)</span>和<span class="math inline">\(P\{\xi_i=0\} = q,p + q = 1\)</span>, 则对于： <span class="math display">\[S_n = \xi_1+\xi_2+...+\xi_n\]</span> 根据期望的基本性质，<span class="math inline">\(S_n\)</span>的数学期望为 <span class="math display">\[ES_n = E(\xi_1)+E(\xi_2) + ... + E(\xi_n) = np \]</span> 证明的过程比第一个证明要简单快捷。</p>
<h3 id="二项分布的方差">3.二项分布的方差</h3>
<p>随机变量<span class="math inline">\(\xi\)</span>,如果<span class="math inline">\(E(\xi-E\xi)^2\)</span>存在，则称它为随机变量<span class="math inline">\(\xi\)</span>的方差（variance）. 并记为<span class="math inline">\(D\xi\)</span>，而<span class="math inline">\(\sqrt{D\xi}\)</span>称为标准差（standard deviation），描述的是随机变量 对其数学期望的偏离程度（dispersion）。</p>
<p><span class="math display">\[ E(X) = np\]</span></p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}

E(X^2) &amp;= \sum_{k=0}^{n} k^2 C_n^kp^kq^{n-k} \\
&amp;=\sum_{k=1}^{n} [k(k-1)+k]\frac{n!}{k!(n-k)!}p^kq^{n-k} \\
&amp;=\sum_{k=2}^{n} \frac{n!}{(k-2)!(n-k)!} + E(X) \\
&amp;=n(n-1)p^2 \sum_{k=2}^{n} \frac{(n-2)!}{(k-2)![(n-2) - (k-2)]!} \cdot p^{k-2}q^{(n-2)-(k-2)} +E(X) \\
&amp;=n(n-1)p^2 \sum_{k{&#39;}=0}^{n-2} C_{n-2}^k{&#39;}p^k{&#39;}q^{(n-2)-k{&#39;}} + E(X) \\
&amp;=n(n-1)p^2 + np \\
&amp;=n^2p^2 + np(1-p) \\

\end{aligned}
\end{equation}\]</span></p>
<p>由于方差恒等式<span class="math inline">\(D(X) = E(X^2) - [E(X)]^2\)</span>，所以 <span class="math inline">\(D(X) = np(1-p)\)</span></p>
<p>二项分布方差证明二：</p>
<p>设<span class="math inline">\(\xi\)</span>是伯努利随机变量，以概率<span class="math inline">\(P\{\xi_i=1\} = p\)</span>和<span class="math inline">\(P\{\xi_i=0\} = q,p + q = 1\)</span>, 根据方差的定义:</p>
<p>伯努利期望<span class="math inline">\(E\xi = p\)</span>, <span class="math display">\[ D\xi = E(\xi - E\xi)^2 = E(\xi - p)^2=(1-p)^2p + (0-p)^q = pq \]</span> 由此可见，<span class="math inline">\(\xi_1,\xi_2,...\xi_n\)</span>是独立同分布的伯努利随机变量序列，且<span class="math inline">\(S_n = \xi_1+\xi_2+...+\xi_n\)</span> 则 <span class="math display">\[DS_n = npq\]</span> 这里参考了方差的性质：如果<span class="math inline">\(\xi\)</span>和<span class="math inline">\(\eta\)</span>独立，则和<span class="math inline">\(\xi+\eta\)</span>的方差等于方差之和。 <span class="math display">\[D(\xi+\eta) = D\xi + D\eta\]</span></p>
]]></content>
      <categories>
        <category>probability</category>
        <category>binomial</category>
      </categories>
      <tags>
        <tag>binomial distribution</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2021/01/15/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>mathematical_symbol</title>
    <url>/2021/02/04/mathematical-symbol/</url>
    <content><![CDATA[<h1 id="希腊字母">希腊字母</h1>
<p><span class="math display">\[
\begin{array}{|c|c|c|c|c|c|c|c|}
\hline
{\alpha} &amp; {\backslash alpha} &amp; {\theta} &amp; {\backslash theta} &amp; {o} &amp; {o} &amp; {\upsilon} &amp; {\backslash upsilon} \\\\
\hline
{\beta} &amp; {\backslash beta} &amp; {\vartheta} &amp; {\backslash vartheta} &amp; {\pi} &amp; {\backslash pi} &amp; {\phi} &amp; {\backslash phi} \\\\
\hline
{\gamma} &amp; {\backslash gamma} &amp; {\iota} &amp; {\backslash iota} &amp; {\varpi} &amp; {\backslash varpi} &amp; {\varphi} &amp; {\backslash varphi} \\\\
\hline
{\delta} &amp; {\backslash delta} &amp; {\kappa} &amp; {\backslash kappa} &amp; {\rho} &amp; {\backslash rho} &amp; {\chi} &amp; {\backslash chi} \\\\
\hline
{\epsilon} &amp; {\backslash epsilon} &amp; {\lambda} &amp; {\backslash lambda} &amp; {\varrho} &amp; {\backslash varrho} &amp; {\psi} &amp; {\backslash psi} \\\\
\hline
{\varepsilon} &amp; {\backslash varepsilon} &amp; {\mu} &amp; {\backslash mu} &amp; {\sigma} &amp; {\backslash sigma} &amp; {\omega} &amp; {\backslash omega} \\\\
\hline
{\zeta} &amp; {\backslash zeta} &amp; {\nu} &amp; {\backslash nu} &amp; {\varsigma} &amp; {\backslash varsigma} &amp; {} &amp; {} \\\\
\hline
{\eta} &amp; {\backslash eta} &amp; {\xi} &amp; {\backslash xi} &amp; {\tau} &amp; {\backslash tau} &amp; {} &amp; {} \\\\
\hline
{\Gamma} &amp; {\backslash Gamma} &amp; {\Lambda} &amp; {\backslash Lambda} &amp; {\Sigma} &amp; {\backslash Sigma} &amp; {\Psi} &amp; {\backslash Psi} \\\\
\hline
{\Delta} &amp; {\backslash Delta} &amp; {\Xi} &amp; {\backslash Xi} &amp; {\Upsilon} &amp; {\backslash Upsilon} &amp; {\Omega} &amp; {\backslash Omega} \\\\
\hline
{\Omega} &amp; {\backslash Omega} &amp; {\Pi} &amp; {\backslash Pi} &amp; {\Phi} &amp; {\backslash Phi} &amp; {} &amp; {} \\\\
\hline
\end{array}
\]</span></p>
]]></content>
      <tags>
        <tag>math</tag>
      </tags>
  </entry>
  <entry>
    <title>normal_distribution</title>
    <url>/2021/01/23/normal-distribution/</url>
    <content><![CDATA[<h1 id="正态分布">正态分布</h1>
<p><span class="math display">\[f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^\frac{(x-\mu)^2}{2\sigma^2}\]</span></p>
<h1 id="正太分布的推导">正太分布的推导</h1>
<p>这里介绍高斯的推导方法，采用误差</p>
<p><span class="math display">\[\begin{align*}
L(\theta) &amp;= L(\theta;x_1,\cdots,x_n)=f(e_1)\cdots f(e_n) \\
&amp; = f(x_1-\theta)\cdots f(x_n-\theta)
\end{align*}\]</span></p>
<p><span class="math inline">\(\frac{d \log L(\theta)}{d \theta} = 0\)</span></p>
<p><span class="math inline">\(\sum_{i=1}^n \frac{f&#39;(x_i-\theta)}{f(x_i-\theta)} = 0\)</span></p>
<p>令 <span class="math inline">\(g(x) = \frac{f&#39;(x)}{f(x)}\)</span></p>
<p><span class="math inline">\(\sum_{i=1}^n g(x_i-\theta) = 0\)</span></p>
<p><span class="math display">\[\begin{equation}
\sum_{i=1}^n g(x_i-\bar{x}) = 0 \quad  (1)
\end{equation}\]</span></p>
<ol type="1">
<li>式中取 <span class="math inline">\(n=2\)</span>, 有</li>
</ol>
<p><span class="math inline">\(g(x_1-\bar{x}) + g(x_2-\bar{x}) = 0\)</span></p>
<p>由于此时有 <span class="math inline">\(x_1-\bar{x} = -(x_2-\bar{x})\)</span></p>
]]></content>
  </entry>
  <entry>
    <title>possion_distribution</title>
    <url>/2021/01/19/possion-distribution/</url>
    <content><![CDATA[<p>泊松分布，二项分布的极限形式，广泛应用于管理科学，运筹学，自然科学中。生活中某个十字路口在一定时间内经过的车辆数，就服从泊松分布。 它描述的是在单位时间（空间）内随机事件发生的次数。 泊松分布的推导如下：</p>
<h1 id="二项分布binomial-distribution">二项分布(binomial distribution)</h1>
<p>两点分布重复n次，就得到了二项分布，二项分布的概率质量函数(probability mass function, PMF): n为实验的总次数，k为实验成功的次数，p是成功的概率 <span class="math display">\[ P(X=k)=C_n^kp^k(1-p)^{n-k} \]</span> 服从二项分布的随机变量记为 <span class="math inline">\(X \sim B(n,p)\)</span></p>
<h2 id="二项分布的期望">二项分布的期望</h2>
<p>二项分布分布，事件发生的概率为p, 不发生的概率为q=1-p, 这里的 <span class="math inline">\(C_n^k\)</span> 称为二项系数，根据二项展开式的系数，可以反推二项分布的概率和为1. <span class="math display">\[ \sum_{k=0}^nP(X=k) = \sum_{k=0}^nC_n^kp^k(1-p)^{n-k} = (p+(1-p))^n=1 \]</span> 期望是离散型随机变量的特征之一，定义如下： 设<span class="math inline">\(\xi\)</span> 为离散型随机变量，它可以取值<span class="math inline">\(x_1,x_2,x_3,...\)</span>，对应的概率为<span class="math inline">\(p_1,p_2,p_3,...\)</span> 如果级数 <span class="math display">\[\sum_{i=1}^{\infty}x_ip_i\]</span> 绝对收敛，则把它称为<span class="math inline">\(\xi\)</span>的数学期望（mathematical expectation）,简称期望，期望值或均值（mean）,记为<span class="math inline">\(E\xi\)</span> 当<span class="math inline">\(\sum_{i=1}^{\infty}{\vert}x_i{\vert}p_i\)</span> 发散时，则<span class="math inline">\(\xi\)</span>的数学期望不存在。</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\sum_{k=0}^{n}kp_k &amp;= \sum_{k=1}{n}{n \choose k}p^kq^{n-k} \\
&amp;= np \sum_{k=1}^{n}{n-1 \choose k-1}p^{k-1}q^{n-k} \\
&amp;=np(p+q)^{n-1} \\
&amp;=np 
\end{aligned}
\end{equation}\]</span></p>
<h2 id="二项分布的方差">二项分布的方差</h2>
<p>随机变量<span class="math inline">\(\xi\)</span>,如果<span class="math inline">\(E(\xi-E\xi)^2\)</span>存在，则称它为随机变量<span class="math inline">\(\xi\)</span>的方差（variance）. 并记为<span class="math inline">\(D\xi\)</span>，而<span class="math inline">\(\sqrt{D\xi}\)</span>称为标准差（standard deviation），描述的是随机变量 对其数学期望的偏离程度（dispersion）。</p>
<p><span class="math display">\[ E(X) = np\]</span></p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}

E(X^2) &amp;= \sum_{k=0}^{n} k^2 C_n^kp^kq^{n-k} \\
&amp;=\sum_{k=1}^{n} [k(k-1)+k]\frac{n!}{k!(n-k)!}p^kq^{n-k} \\
&amp;=\sum_{k=2}^{n} \frac{n!}{(k-2)!(n-k)!} + E(X) \\
&amp;=n(n-1)p^2 \sum_{k=2}^{n} \frac{(n-2)!}{(k-2)![(n-2) - (k-2)]!} \cdot p^{k-2}q^{(n-2)-(k-2)} +E(X) \\
&amp;=n(n-1)p^2 \sum_{k{&#39;}=0}^{n-2} C_{n-2}^k{&#39;}p^k{&#39;}q^{(n-2)-k{&#39;}} + E(X) \\
&amp;=n(n-1)p^2 + np \\
&amp;=n^2p^2 + np(1-p) \\

\end{aligned}
\end{equation}\]</span></p>
<p>由于方差恒等式<span class="math inline">\(D(X) = E(X^2) - [E(X)]^2\)</span>，所以 <span class="math inline">\(D(X) = np(1-p)\)</span></p>
<h1 id="泊松分布possion-distribution">泊松分布(possion distribution)</h1>
<p>在下面的情形下，<span class="math inline">\(n \to \infty,\,p,\,\lambda &gt; 0,\,\lambda=np,\,p = \frac{\lambda}{n}\)</span> <span class="math display">\[P(X=k)=\displaystyle \lim_{n \to \infty} C_n^k \cdot (\frac{\lambda}{n})^k \cdot (1-\frac{\lambda}{n})^{n-k}\]</span> <span class="math display">\[C_n^k \cdot (\frac{\lambda}{n})^k \cdot (1-\frac{\lambda}{n})^{n-k}=\frac{1}{k!} \cdot \frac{n(n-1)...(n-k+1)}{n^k} \cdot \lambda^k \cdot\frac{(1-\frac{\lambda}{n})^n}{(1-\frac{\lambda}{n})^k}\]</span> 因为<span class="math inline">\(n \to \infty\)</span>, k不变，第二个因子</p>
<p><span class="math display">\[\displaystyle \lim_{n \to \infty} \frac{n(n-1)...(n-k+1)}{n^k}=\frac{n}{n} \cdot \frac{n-1}{n}...\frac{n-(k-1)}{n}=1\]</span></p>
<p>第四个因子中的分母</p>
<p><span class="math display">\[\displaystyle \lim_{n \to \infty}(1-\frac{\lambda}{n})^k = 1\]</span></p>
<p>第四个因子中的分子 <span class="math display">\[(1-\frac{\lambda}{n})^n = [(1-\frac{\lambda}{n})^{-\frac{n}{\lambda}}]^{-\lambda}\]</span> 因为 <span class="math inline">\(\displaystyle \lim_{n \to \infty} (1-\frac{\lambda}{n})^{-\frac{n}{\lambda}} = e\)</span>，所以 <span class="math inline">\((1-\frac{\lambda}{n})^n = e^{-\lambda}\)</span></p>
<p>因此 <span class="math display">\[\displaystyle \lim_{n \to \infty} C_n^k \cdot (\frac{\lambda}{n})^k \cdot (1-\frac{\lambda}{n})^{n-k}=\frac{\lambda^k}{k!}e^{-\lambda}\]</span> 综上 <span class="math display">\[P(X=k)=\frac{\lambda^k}{k!}e^{-\lambda},k=0,1,2...\]</span> 服从泊松分布随机变量记为 <span class="math inline">\(X \sim p(\lambda)\)</span> 在应用中，当p相当小（p &lt;= 0.1）时，我们用下面的近似公式 <span class="math inline">\(b(k;n,p) \sim \frac{(np)^k}{k!}e^{-np}\)</span></p>
<h2 id="泊松分布期望">泊松分布期望</h2>
<p>证明泊松分布的概率和为1 <span class="math display">\[ \sum_{k=0}^{\infty}p(k;\lambda) = \sum_{k=0}^{\infty} \frac{\lambda^k}{k!}e^{-\lambda}=e^{-\lambda} \cdot e^{\lambda}=1\]</span> <span class="math inline">\(e^{\lambda} = \sum_{k=0}^{\infty} \frac{\lambda^k}{k!}\)</span> 为指数函数的泰勒展开式</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\sum_{k=0}^{\infty}kp_k &amp;= \sum_{k=1}^{\infty}k \cdot \frac{\lambda^k}{k!}e^{-\lambda} \\
&amp;=\lambda e^{-\lambda}\sum_{k=1}^{\infty}\frac{\lambda^{k-1}}{(k-1)!} \\
&amp;=\lambda e^{-\lambda} \cdot e^\lambda \\
&amp;=\lambda

\end{aligned}
\end{equation}\]</span></p>
<h2 id="泊松分布方差">泊松分布方差</h2>
<p>方差的推导如下： 对于泊松分布期望：<span class="math inline">\(E(\xi)=\lambda\)</span></p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
E(\xi)^2 &amp;= \sum k^2p_k \\
&amp;=\sum_{k=1}^{\infty}k^2 \cdot \frac{\lambda^k}{k!}e^{-\lambda} \\
&amp;=\sum_{k=1}^{\infty}k \frac{\lambda^k}{(k-1)!} e^{-\lambda} \\
&amp;=\sum_{k=1}^{\infty} [(k-1)+1] \cdot \frac{\lambda^k}{(k-1)!} e^{-\lambda} \\
&amp;=\sum_{k=2}^{\infty} \frac{\lambda^k}{(k-2)!}e^{-\lambda} + \sum_{k=1}^{\infty} \frac{\lambda^k}{(k-1)!}e^{-\lambda} \\
&amp;=\lambda^2\sum_{k{&#39;}=0}^{\infty} \frac{\lambda^k{&#39;}}{(k{&#39;})!}e^{-\lambda} + \lambda\sum_{k{&#39;&#39;}=0}^{\infty} \frac{\lambda^k{&#39;&#39;}}{(k{&#39;&#39;})!}e^{-\lambda} \\
&amp;=\lambda^2+\lambda
\end{aligned}
\end{equation}\]</span></p>
<p><span class="math display">\[D\xi = E(\xi^2) - [E(\xi)]^2 = \lambda^2+\lambda -\lambda^2 = \lambda\]</span> 附方差恒等式的证明 <span class="math display">\[D\xi = E(\xi^2) - [E(\xi)]^2 \]</span></p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}

D(\xi) &amp;= E[\xi - E(\xi)]^2 \\
&amp;=E{\xi^2 - 2E(\xi) \cdot \xi +[E(\xi)]^2} \\
&amp;=E(\xi^2) - E[2E(\xi) \cdot \xi] + E[E(\xi)]^2 \\
&amp;=E(\xi^2) - 2E(\xi) \cdot E(\xi) + [E(\xi)]^2\\
&amp;=E(\xi^2) - [E(\xi)]^2

\end{aligned}
\end{equation}\]</span></p>
<p>方差恒等式证明二： 以<span class="math inline">\(\xi \sim p(\xi)\)</span>为例</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}

D(\xi) &amp;= \int_{-\infty}^{\infty}[\xi - E(\xi)]^2p(\xi)d\xi \\
&amp;=\int_{-\infty}^{\infty}\{\xi^2 - 2E(\xi) \cdot \xi + [E(\xi)]^2\}p(\xi)d\xi \\
&amp;=\int_{-\infty}^{\infty}\xi^2p(\xi)d\xi - 2E(\xi)\int_{-\infty}^{\infty}\xi p(\xi)d\xi + [E(\xi)]^2\int_{-\infty}^{\infty}p(\xi)d\xi \\
&amp;=E(\xi^2) - 2E(\xi) \cdot E(\xi) + [E(\xi)]^2 \cdot 1 \\
&amp;=E(\xi^2) - [E(\xi)]^2

\end{aligned}
\end{equation}\]</span></p>
]]></content>
      <categories>
        <category>probability</category>
        <category>possion</category>
      </categories>
      <tags>
        <tag>二项分布</tag>
        <tag>泊松分布</tag>
      </tags>
  </entry>
  <entry>
    <title>伯努利大数定理</title>
    <url>/2021/02/03/%E4%BC%AF%E5%8A%AA%E5%88%A9%E5%A4%A7%E6%95%B0%E5%AE%9A%E7%90%86/</url>
    <content><![CDATA[<h1 id="伯努利大数定理law-of-large-numbers">伯努利大数定理(Law of Large Numbers)</h1>
<p>伯努利分布也称两点分布,相关的描述和证明见<a href="http://www.zhusitao.cn/2021/01/25/bernoulli-distribution/">伯努利分布</a></p>
<h2 id="伯努利概型">伯努利概型</h2>
<p><span class="math inline">\(\Omega\)</span>,<span class="math inline">\(\mathscr{A}\)</span>,<span class="math inline">\(P\)</span> ,其中 <span class="math inline">\(\Omega=\{\omega:\omega=(a_1,...a_n),a_i=0,1\}\)</span></p>
<p><span class="math inline">\(\mathscr{A}=\{A:A \subset \Omega\}\)</span> , <span class="math inline">\(P(\{\omega\})=p^{\sum a_i}(1-p)^{n-\sum a_i}=p(\omega)\)</span></p>
<p>上面定义的三对象，称作伯努利概型。白话就是“有两种结局的n次独立试验的概率模型”</p>
<p>对于二项分布而言，<span class="math inline">\(ES_n = np\)</span> <span class="math inline">\(E\frac{S_n}{n} = p\)</span>,即成功的频率<span class="math inline">\(S_n/n\)</span>的平均值等于成功的概率。那么成功的频率对成功的概率的偏差如何呢？ 这里需要借助切比雪夫不等式。</p>
<h2 id="切比雪夫不等式">切比雪夫不等式</h2>
<p>设(<span class="math inline">\(\Omega\)</span>,<span class="math inline">\(\mathscr{A}\)</span>,<span class="math inline">\(P\)</span>)是某一概率空间，<span class="math inline">\(\xi=\xi(\omega)\)</span> 是非负随机变量，那么对于任意的<span class="math inline">\(\varepsilon&gt;0\)</span>,</p>
<p><span class="math display">\[
P\{\xi\geq\varepsilon\} \leq \frac{E\xi}{\varepsilon}
\]</span></p>
<p>切比雪夫不等式证明： 根据示性函数的性质，</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}

\xi &amp;=\xi I(\xi \geq \varepsilon) + \xi I(\xi &lt; \varepsilon) \\
&amp;\geq \xi I(\xi \geq \varepsilon) \\
&amp;\geq \varepsilon I(\xi \geq \varepsilon) \\

\end{aligned}
\end{equation}\]</span></p>
<p>所以 <span class="math inline">\(\xi \geq \varepsilon I(\xi \geq \varepsilon)\)</span> 根据期望的性质 <span class="math display">\[\begin{equation}
\begin{aligned}
E\xi &amp;\geq E\varepsilon I(\xi \geq \varepsilon) \\
&amp;\geq \varepsilon E I(\xi \geq \varepsilon) = \varepsilon P\{\xi \geq \varepsilon\} 
\end{aligned}
\end{equation}\]</span> 不等式得证。</p>
<h2 id="切比雪夫不等式的另一种表达方式">切比雪夫不等式的另一种表达方式</h2>
<p>设随机变量<span class="math inline">\(X\)</span>的数学期望和方差都存在，对于任意的常数<span class="math inline">\(\varepsilon\)</span>有： <span class="math display">\[
P(|X-E(X)| \geq \varepsilon) \leq \frac{Var(X)}{\varepsilon^2}
\]</span> 或者 <span class="math display">\[
P(|X-E(X)| \leq \varepsilon) \geq 1- \frac{Var(X)}{\varepsilon^2}
\]</span> 证明如下： 设<span class="math inline">\(X\)</span>是一个连续的随机变量，其密度函数为<span class="math inline">\(p(x)\)</span>,记<span class="math inline">\(E(X)=a\)</span></p>
<p><span class="math display">\[\begin{align*}
P(|X-a|\geq\varepsilon) &amp;= \int\limits_{\{x:|x-a|\geq\varepsilon\}} \quad p(x)dx \\
&amp;\leq \int \limits_{\{x:|x-a|\geq\varepsilon\}} \quad \frac{(x-a)^2}{\varepsilon^2}p(x)dx \\
&amp;\leq \frac{1}{\varepsilon^2}\int_{-\infty}^{+\infty}(x-a)^2p(x)dx = \frac{Var(x)}{\varepsilon^2} 

\end{align*}\]</span></p>
<p>此证明的第一个不等式，是由于 <span class="math inline">\(|X-E(X)| \geq \varepsilon\)</span>得出，第二个不等式是积分区间的扩大导致的。 该切比雪夫不等式的证明，将<span class="math inline">\(\xi = X-E(X)\)</span>,则就是第一个切比雪夫不等式的证明推导。</p>
<h1 id="证明伯努利概型大数定理">证明伯努利概型大数定理</h1>
<p><span class="math display">\[
\lim_{n \to \infty}P(|\frac{S_n}{n}-p|\le \varepsilon) = 1
\]</span></p>
<p>证明:</p>
<p><span class="math display">\[\begin{align*}

1 &amp;\geq P(|\frac{S_n}{n}-p| \leq \varepsilon) \\
&amp;\geq 1 - \frac{Var(\frac{S_n}{n})}{\varepsilon^2} = 1- \frac{p(1-p)}{n^2\varepsilon^2}

\end{align*}\]</span></p>
<p>当<span class="math inline">\(n\)</span>趋近与<span class="math inline">\(\infty\)</span>时，上式趋近与1,事件发生的频率趋近与概率。</p>
]]></content>
      <tags>
        <tag>大数定理</tag>
        <tag>Law of Large Numbers</tag>
      </tags>
  </entry>
  <entry>
    <title>github.io 博客搭建指南</title>
    <url>/2021/01/15/%E6%90%AD%E5%BB%BAgithub.io%E5%8D%9A%E5%AE%A2%E6%8C%87%E5%8D%97/</url>
    <content><![CDATA[<p>Hexo是一款基于Node.js的静态博客框架，依赖少易于安装使用，可以方便的生成静态网页托管在GitHub和Heroku上，是搭建博客的首选框架。</p>
<h1 id="搭建步骤">搭建步骤</h1>
<ol type="1">
<li>获取域名</li>
<li>建立github仓库</li>
<li>安装git</li>
<li>安装Node.js</li>
<li>安装Hexo</li>
<li>推送网站</li>
<li>绑定域名</li>
<li>更换主题</li>
</ol>
<p><span class="math display">\[E[X]=x^4\]</span></p>
]]></content>
  </entry>
  <entry>
    <title>期望和方差的性质及证明</title>
    <url>/2021/02/02/%E6%9C%9F%E6%9C%9B%E5%92%8C%E6%96%B9%E5%B7%AE%E7%9A%84%E6%80%A7%E8%B4%A8%E5%8F%8A%E8%AF%81%E6%98%8E/</url>
    <content><![CDATA[<h3 id="期望数学定义及性质证明">1.期望数学定义及性质证明</h3>
<p>在n次独立重复的试验中观察随机变量<span class="math inline">\(\xi\)</span>的取值，则取<span class="math inline">\(x_i\)</span>的值大致 应该出现在<span class="math inline">\(np_i\)</span>次，根据n次试验结果计算该随机变量的平均值大致如下： <span class="math display">\[
\frac{1}{n}[np_1x_1+np_2x_2+...+np_kx_k] = \sum_{i=1}^{k}p_ix_i
\]</span> 定义：实数<span class="math inline">\(E\xi = \sum_{i=1}^{k}x_iP(A_i)\)</span>,称作随机变量<span class="math inline">\(\xi=\sum_{i=1}^{k}x_iI(A_i)\)</span> 的数学期望或平均值。 数学期望的基本性质</p>
<ol type="1">
<li>若<span class="math inline">\(\xi \ge 0\)</span>,则<span class="math inline">\(E\xi \ge 0\)</span>.</li>
<li><span class="math inline">\(E(a\xi+b\eta) = aE\xi + bE\eta\)</span>,其中a,b为常数.</li>
<li>若<span class="math inline">\(\xi \ge \eta\)</span>,则<span class="math inline">\(E\xi \ge E\eta\)</span>.</li>
<li><span class="math inline">\(|E\xi| \le E|\xi|\)</span>.</li>
<li>若<span class="math inline">\(\xi\)</span>和<span class="math inline">\(\eta\)</span>独立,则<span class="math inline">\(E\xi\eta=E\xi \cdot E\eta\)</span>.</li>
<li><span class="math inline">\((E|\xi|)^2 \le E{\xi}^2 \cdot E{\eta}^2\)</span>,柯西-瓦尔茨不等式.</li>
<li>若<span class="math inline">\(\xi = I(A)\)</span>,则<span class="math inline">\(E\xi = P(A)\)</span>.</li>
</ol>
<p>证明： 性质1显然成立 性质7，根据示性函数，由于 <span class="math display">\[
\xi = I_A(\omega)= \begin{cases}
1,  \omega \in A, \\
0,  \omega \in A.
\end{cases}
\]</span> 由于伯努利试验只有0，1，所以根据随机变量期望的定义，性质7成立。</p>
<p>性质2的证明： 设 <span class="math inline">\(\xi=\sum x_iI(A_i), \eta=\sum y_jI(B_j)\)</span>,</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
a\xi+b\eta &amp;= a\sum_{i,j}x_i I(A_i \cap B_j) + b\sum_{i,j} y_j I(A_i \cap B_j) \\ 
&amp;=\sum_{i,j}(a x_i+b y_j)I(A_i \cap B_j) \\
\end{aligned}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
E(a\xi+b\eta) &amp;= \sum_{i,j}(a x_i+b y_i)P(A_i \cap B_j) \\ 
&amp;=\sum_{i}a x_iP(A_i) + \sum_{j}b y_jP(B_j) \\ 
&amp;=a\sum_{i}x_iP(A_i) + b\sum_{j}y_jP(B_j) \\ 
&amp;=a\sum(\xi) + b\sum(\eta) \\
\end{aligned}
\end{equation}\]</span></p>
<p>性质3 的证明： 由性质1，2可以证明3。</p>
<p>性质4的证明： <span class="math display">\[
|E\xi| = |\sum_{i}x_iP(A_i)| \le \sum_{i}|x_i|P(B_j)=E|\xi|
\]</span></p>
<p>性质5的证明： <span class="math display">\[\begin{equation}
\begin{aligned}
E\xi\eta &amp;= E(\sum_{i}x_i I(A_i))(\sum_{j}y_j I(B_j)) \\
&amp;=E\sum_{i,j}x_iy_jI(A_i \cap B_j) \\ 
&amp;=\sum_{i,j}x_i y_jP(A_i \cap B_j) \\
&amp;=\sum x_j y_jP(A_i)P(B_j) \\
&amp;=(\sum_{i}x_i P(A_i))(\sum_{j}y_i P(B_j)) \\
&amp;=E\xi \cdot E\eta \\
\end{aligned}
\end{equation}\]</span> 证明过程中，对于独立的随机变量<span class="math inline">\(\xi\)</span> ,<span class="math inline">\(\eta\)</span>， 事件 <span class="math inline">\(A_i = {\omega:\xi(\omega)=x_i}\)</span> 和 <span class="math inline">\(B_j={\omega:\eta(\omega)=y_j}\)</span> 事件独立：<span class="math inline">\(P(A_i \cap B_j) = P(A_i)P(B_j)\)</span></p>
<p>性质6 的证明： 已知 <span class="math display">\[ \xi^2 = \sum_{i=1}^{l}x_i^2 I(A_i), \eta^2 = \sum y_i^2 I(b_j)\]</span> <span class="math display">\[ E\xi^2 = \sum_{i=1}^l x_i^2 P(A_i), E\eta^2 = \sum_{j=1}^k y_i^2P(B_j) \]</span></p>
<p>设 <span class="math inline">\(E\xi^2 &gt; 0\)</span> ;<span class="math inline">\(E\eta^2 &gt; 0\)</span>, 记 <span class="math inline">\(\widetilde{\xi} = \frac{\xi}{\sqrt{E\xi^2}}\)</span> ,<span class="math inline">\(\widetilde{\eta} = \frac{\eta}{\sqrt{E\eta^2}}\)</span></p>
<p>由于 <span class="math inline">\(2|\widetilde{\xi} \widetilde{\eta}| \le {\widetilde{\xi}}^2 + {\widetilde{\eta}}^2\)</span> 可得 <span class="math inline">\(2E|\widetilde{\xi} \widetilde{\eta}| \le E{\widetilde{\xi}}^2 + E {\widetilde{\eta}}^2 = 1+1=2\)</span> 所以 <span class="math inline">\(E|\widetilde{\xi} \widetilde{\eta}| \le 1\)</span> <span class="math display">\[\begin{equation}
\begin{aligned}
(E|\xi\eta|)^2 = (E|\widetilde{\xi}\sqrt{E\xi^2} \widetilde{\eta}\sqrt{E\eta^2}|)^2 \\ 
&amp;=(E|\sqrt{E\xi^2E\eta^2}\widetilde{\xi}\widetilde{\eta}|)^2 \\ 
&amp;=E\xi^2 E\eta^2(E|\widetilde{\xi}\widetilde{\eta}|)^2 \\ 
\end{aligned}
\end{equation}\]</span> 由于 <span class="math inline">\(E|\widetilde{\xi} \widetilde{\eta}| \le 1\)</span> 所以 <span class="math inline">\((E|\widetilde{\xi} \widetilde{\eta}|)^2 \le 1\)</span> 因此 <span class="math inline">\((E|\xi\eta|)^2 = E\xi^2 E\eta^2(E|\widetilde{\xi}\widetilde{\eta}|)^2 \le E\xi^2 \times E\eta^2\)</span> 所以性质6 得证。</p>
<p>因为设 <span class="math inline">\(E\xi &gt; 0\)</span>,所以还有一种特殊情况要证明。 假设 <span class="math inline">\(E\xi^2 = 0\)</span> , <span class="math inline">\(\sum_{i}x_i^2P(A_i) = 0\)</span> 因此0是<span class="math inline">\(\xi\)</span>的可能的值，且 <span class="math inline">\(P{\omega : \xi(\omega) = 0} = 1\)</span> 因为<span class="math inline">\(E\xi^2\)</span> 或者 <span class="math inline">\(E\eta^2=0\)</span> ,显然 <span class="math inline">\(E|\xi\eta| = 0\)</span> ,性质6依然成立。</p>
<h3 id="方差的定义及性质证明">2.方差的定义及性质证明</h3>
<p>方差是来描述数据分散程度的指标</p>
<p><span class="math display">\[D\xi = E(\xi - E\xi)^2\]</span> <span class="math inline">\(D\xi\)</span>记为方差，<span class="math inline">\(\sigma = \sqrt{D\xi}\)</span>记为标准差。</p>
<p>性质1 由于 <span class="math display">\[
E(\xi-E\xi)^2 = E[\xi^2 - 2\xi E\xi + (E\xi)^2] = E(\xi^2) - (E\xi)^2
\]</span> 所以 <span class="math display">\[D\xi = E\xi^2 - (E\xi)^2\]</span></p>
<p>根据方差的定义，可知<span class="math inline">\(D\xi\ge0\)</span></p>
<p>性质2 对于任意常数的a,b <span class="math inline">\(D(a+b\xi) = b^2D\xi\)</span> 当<span class="math inline">\(Da=0\)</span>,<span class="math inline">\(D(b\xi) = b^2D\xi\)</span></p>
<p>性质3 对于二个随机变量<span class="math inline">\(\xi\)</span>,<span class="math inline">\(\eta\)</span></p>
<p><span class="math display">\[D(\xi+\eta) = E[(\xi-E\xi)+(\eta - E\eta)]^2 = D\xi + D\eta +2E(\xi-E\xi)(\eta-E\eta)\]</span></p>
<p>记： <span class="math display">\[
cov(\xi,\eta) = E(\xi-E\xi)(\eta-E\eta)
\]</span> 称作随机变量<span class="math inline">\(\xi\)</span>和<span class="math inline">\(\eta\)</span>的协方差，如果<span class="math inline">\(D\xi \ge 0\)</span>, <span class="math inline">\(D\eta \ge 0\)</span>, 则 <span class="math inline">\(\rho(\xi,\eta) = \frac{cov(\xi,\eta)}{\sqrt{D\xi \times D\eta}}\)</span> 称作随机变量<span class="math inline">\(\xi\)</span>和<span class="math inline">\(\eta\)</span>的相关系数。</p>
]]></content>
      <tags>
        <tag>expeactation</tag>
        <tag>variance</tag>
      </tags>
  </entry>
  <entry>
    <title>超几何分布</title>
    <url>/2021/02/05/%E8%B6%85%E5%87%A0%E4%BD%95%E5%88%86%E5%B8%83/</url>
    <content><![CDATA[<h1 id="超几何分布hypergeometric-distribution">超几何分布（hypergeometric distribution）</h1>
<p>设有N个产品，其中次品有M个。从中任取n个（n&lt;=N-M） 则，n个抽取的产品中次品数目X是离散型随机变量， 概率质量函数如下： <span class="math display">\[P(X=m)=\frac{C_M^m \cdot C_{N-M}^{n-m}}{C_N^n}, (m=0,1,2,...min(M,n))\]</span> 则称 X服从参数为n,m和N的超几何分布，记为 <span class="math inline">\(X \sim H(n, m, N)\)</span> $$ =  </p>
]]></content>
      <tags>
        <tag>超几何分布</tag>
        <tag>概率论</tag>
      </tags>
  </entry>
  <entry>
    <title>分位数标准化</title>
    <url>/2021/02/05/%E5%88%86%E4%BD%8D%E6%95%B0%E6%A0%87%E5%87%86%E5%8C%96/</url>
    <content><![CDATA[<h1 id="分位数标准化-quantile-normalization">分位数标准化 Quantile normalization</h1>
<p>分位数标准化就是使得两个分布在统计属性上相同的技术手法。 该方法在基因芯片中经常使用。</p>
<h1 id="例子">例子</h1>
<p>如图我们有三个基因芯片的数据，假设有A,B,C,D四个基因</p>
<p>表格Table1</p>
<table>
<thead>
<tr class="header">
<th>Gene</th>
<th>Sample1</th>
<th>Sample2</th>
<th>Sample3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A</td>
<td>5</td>
<td>4</td>
<td>3</td>
</tr>
<tr class="even">
<td>B</td>
<td>2</td>
<td>1</td>
<td>4</td>
</tr>
<tr class="odd">
<td>C</td>
<td>3</td>
<td>4</td>
<td>6</td>
</tr>
<tr class="even">
<td>D</td>
<td>4</td>
<td>2</td>
<td>8</td>
</tr>
</tbody>
</table>
<p>对每一列进行排序，按照从小到大的顺序。 表格Table2</p>
<table>
<thead>
<tr class="header">
<th>Gene</th>
<th>Sample1</th>
<th>Sample2</th>
<th>Sample3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A</td>
<td>iv</td>
<td>iii</td>
<td>i</td>
</tr>
<tr class="even">
<td>B</td>
<td>i</td>
<td>i</td>
<td>ii</td>
</tr>
<tr class="odd">
<td>C</td>
<td>ii</td>
<td>iii</td>
<td>iii</td>
</tr>
<tr class="even">
<td>D</td>
<td>iii</td>
<td>ii</td>
<td>iv</td>
</tr>
</tbody>
</table>
<p>这个排序好的列表(Table2)后续会用到。回到第一个数据表(Table1)，重排每一列的数据，根据从小到大的顺序依次排列。 第一列原始的数据是5,2,3,4 ---&gt; 2,3,4,5 第二列原始的数据是4,1,4,2 ---&gt; 1,2,4,4 第三列原始的数据是3,4,6,8 ---&gt; 3,4,6,8(原始已经是从小大，不变)</p>
<p>表格Table3</p>
<table>
<thead>
<tr class="header">
<th>Gene</th>
<th>Sample1</th>
<th>Sample2</th>
<th>Sample3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A</td>
<td>2</td>
<td>1</td>
<td>3</td>
</tr>
<tr class="even">
<td>B</td>
<td>3</td>
<td>2</td>
<td>4</td>
</tr>
<tr class="odd">
<td>C</td>
<td>4</td>
<td>4</td>
<td>6</td>
</tr>
<tr class="even">
<td>D</td>
<td>5</td>
<td>4</td>
<td>8</td>
</tr>
</tbody>
</table>
<p>计算Table3每一行的均值，找到新的排序, A (2+1+3)/3 = 2.00 = rank i</p>
<p>B (3+2+4)/3 = 3.00 = rank ii</p>
<p>C (4+4+6)/3 = 4.67 = rank iii</p>
<p>D (5+4+8)/3 = 5.67 = rank iv</p>
<p>根据原始数据的排序Table2，和上面新排序对应的值， 将原始值根据对应的排序进行值的替换。</p>
<p>第一次标准化 表格Table4</p>
<table>
<thead>
<tr class="header">
<th>Gene</th>
<th>Sample1</th>
<th>Sample2</th>
<th>Sample3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A</td>
<td>5.67</td>
<td>4.67</td>
<td>2.00</td>
</tr>
<tr class="even">
<td>B</td>
<td>2.00</td>
<td>2.00</td>
<td>3.00</td>
</tr>
<tr class="odd">
<td>C</td>
<td>3.00</td>
<td>4.67</td>
<td>4.67</td>
</tr>
<tr class="even">
<td>D</td>
<td>4.67</td>
<td>3.00</td>
<td>5.67</td>
</tr>
</tbody>
</table>
<p>注意，在第二列中有并列的值，这些并列的值应该被平均值替换，于是在这里我们替换第二列中并列的值， 使用4.67和5.67的(4.67+5.67)/2=5.17平均值来替换。</p>
<p>第二次标准化 表格Table5</p>
<table>
<thead>
<tr class="header">
<th>Gene</th>
<th>Sample1</th>
<th>Sample2</th>
<th>Sample3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A</td>
<td>5.67</td>
<td>5.17</td>
<td>2.00</td>
</tr>
<tr class="even">
<td>B</td>
<td>2.00</td>
<td>2.00</td>
<td>3.00</td>
</tr>
<tr class="odd">
<td>C</td>
<td>3.00</td>
<td>5.17</td>
<td>4.67</td>
</tr>
<tr class="even">
<td>D</td>
<td>4.67</td>
<td>3.00</td>
<td>5.67</td>
</tr>
</tbody>
</table>
<p>新的值符合相同的分布，我们现在来看看一些统计量，发现这些值比较相近。</p>
<table>
<thead>
<tr class="header">
<th>Sample1</th>
<th>Sample2</th>
<th>Sample3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Min. :2.000</td>
<td>Min. :2.000</td>
<td>Min. :2.000</td>
</tr>
<tr class="even">
<td>1st Qu.:2.750</td>
<td>1st Qu.:2.750</td>
<td>1st Qu.:2.750</td>
</tr>
<tr class="odd">
<td>Median :3.833</td>
<td>Median :4.083</td>
<td>Median :3.833</td>
</tr>
<tr class="even">
<td>Mean :3.833</td>
<td>Mean :3.833</td>
<td>Mean :3.833</td>
</tr>
<tr class="odd">
<td>3rd Qu.:4.917</td>
<td>3rd Qu.:5.167</td>
<td>3rd Qu.:4.917</td>
</tr>
<tr class="even">
<td>Max. :5.667</td>
<td>Max. :5.167</td>
<td>Max. :5.667</td>
</tr>
</tbody>
</table>
<h1 id="python实现quantile-normalize">python实现quantile normalize</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quantileNormalize</span>(<span class="params">df_input</span>):</span></span><br><span class="line">    df = df_input.copy()</span><br><span class="line">    <span class="comment">#compute rank</span></span><br><span class="line">    dic = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> df:</span><br><span class="line">        dic.update(&#123;col : <span class="built_in">sorted</span>(df[col])&#125;)</span><br><span class="line">    sorted_df = pd.DataFrame(dic)</span><br><span class="line">    rank = sorted_df.mean(axis = <span class="number">1</span>).tolist()</span><br><span class="line">    <span class="comment">#sort</span></span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> df:</span><br><span class="line">        t = np.searchsorted(np.sort(df[col]), df[col])</span><br><span class="line">        df[col] = [rank[i] <span class="keyword">for</span> i <span class="keyword">in</span> t]</span><br><span class="line">    <span class="keyword">return</span> df</span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>统计学</tag>
        <tag>标准化</tag>
      </tags>
  </entry>
  <entry>
    <title>统计量和样本方差</title>
    <url>/2021/02/16/%E7%BB%9F%E8%AE%A1%E9%87%8F%E5%92%8C%E6%A0%B7%E6%9C%AC%E6%96%B9%E5%B7%AE/</url>
    <content><![CDATA[<h1 id="统计量">统计量</h1>
<p>不含有任何位置参数的样本函数称为统计量。</p>
<p>例如，数据的算数平均数称为样本均值。</p>
<p><span class="math inline">\(\overline x = \frac{1}{n} \sum_{i=1}^{n}x_i\)</span></p>
<h1 id="估计量">估计量</h1>
<p>用于估计未知参数的统计量称为点估计量。 参数<span class="math inline">\(\theta\)</span>的估计量常用<span class="math inline">\(\hat \theta=\hat \theta(x_1,x_2,...x_n)\)</span>,表示，参数的可能取值范围是参数空间，记为<span class="math inline">\(\Theta = (\theta)\)</span>。</p>
<p>这里的参数常常只如下几种： * 分布中所含有的位置参数； * 分布中的期望，方差，标准差，分位数，和特征值； * 某事件的概率； 一个参数的估计量通常不止一个，如何判断优劣？常用的标准有多个。如无偏性，有效性，均方误差最小，相合性等等。</p>
<p>设<span class="math inline">\(\hat \theta =\hat \theta(x_1,x_2,...x_n)\)</span>是参数<span class="math inline">\(\theta\)</span>的一个估计，对于参数空间<span class="math inline">\(\Theta=\theta\)</span>中任意一个<span class="math inline">\(\theta\)</span> 都有 <span class="math display">\[
E(\hat \theta) = \theta, \forall \theta \in \Theta
\]</span> 则称<span class="math inline">\(\hat \theta\)</span>为<span class="math inline">\(\theta\)</span>的无偏估计，否则称为<span class="math inline">\(\theta\)</span>有偏估计。</p>
<h1 id="样本方差">样本方差</h1>
<p>统计学中常用的三个统计量是：</p>
<ul>
<li>样本均值：<span class="math inline">\(\overline x = \frac{1}{n} \sum_{i=1}^{n}x_i\)</span></li>
<li>样本方差：<span class="math inline">\(s^2 = \frac{1}{n-1} \sum_{i=1}^{n}(x_i-\overline x)\)</span></li>
<li>样本标准差：<span class="math inline">\(s = \sqrt{s^2}\)</span></li>
</ul>
<ol type="1">
<li>样本均值<span class="math inline">\(\overline x\)</span>总位于数据中部，他是总体均值<span class="math inline">\(\mu\)</span>的无偏估计，即<span class="math inline">\(E(\overline x) = \mu\)</span></li>
<li><span class="math inline">\(x_i\)</span>对<span class="math inline">\(\overline x\)</span>的偏差<span class="math inline">\(x_i-\overline x\)</span>可正可负，其和恒为零，<span class="math inline">\(\sum_{i=1}^{n}(x_i-\overline x) = 0\)</span> 这个等式表明: n个偏差中只有n-1个是独立的，第n个可以根据其和为0的公式计算得出。 在统计中独立偏差的个数称为自由度，记为<span class="math inline">\(f\)</span>，故n个偏差有n-1个自由度，即<span class="math inline">\(f= n-1\)</span>。</li>
<li>全部的偏差之和恒为零，故样本偏差只和不能累加起来，不能直接用来度量样本散布大小，而改为样本偏差平方和<span class="math inline">\(Q\)</span>. <span class="math display">\[
Q = \sum_{i=1}^{n}(x_i-\overline x)^2
\]</span></li>
<li><p>在样本量不同的场合，偏差平方和<span class="math inline">\(Q\)</span>失去了比较样本散布大小的公平性。为了消除样本量大小对偏差平方和的干扰，改用平均偏差平方和<span class="math inline">\(s_n^2\)</span>来度量， <span class="math display">\[
s_n^2 = \frac{Q}{n} = \frac{1}{n} \sum_{i=1}^{n}(x_i-\overline x)^2
\]</span> 样本方差<span class="math inline">\(s_n^2\)</span>是总体方差<span class="math inline">\(\sigma^2\)</span>的一个估计。</p></li>
<li><p><span class="math inline">\(s_n^2\)</span>的改进。无论从理论还是实际使用中，用样本方差<span class="math inline">\(s_n^2\)</span>估计总体方差<span class="math inline">\(\sigma^2\)</span>多数情况下是偏小的。 证明如下：</p></li>
</ol>
<p><span class="math display">\[\begin{equation}
\begin{aligned}

s_n^2 &amp; = \frac{1}{n}\sum_{i=1}{n}(x_i-\overline x)^2 \\
&amp;= \frac{1}{n}\sum_{i=1}^{n}(x_i^2 - 2x_i\overline x + \overline x^2) \\
&amp; = \frac{1}{n}\sum_{i=1}^{n}x_i^2 - 2\frac{1}{n}\sum_{i=1}^{n}x_i \cdot \overline x + \overline x^2 \\
&amp; = \frac{1}{n}\sum_{i=1}^{n}x_i^2 - \overline x^2 \\

\end{aligned}
\end{equation}\]</span></p>
<p>为了求<span class="math inline">\(E(s_n^2)\)</span>,先求<span class="math inline">\(E(x_i^2),E(\overline x^2)\)</span>,</p>
<p>根据方差基本性质,<a href="http://www.zhusitao.cn/2021/02/02/%E6%9C%9F%E6%9C%9B%E5%92%8C%E6%96%B9%E5%B7%AE%E7%9A%84%E6%80%A7%E8%B4%A8%E5%8F%8A%E8%AF%81%E6%98%8E/">方差和均值关系证明</a>， 样本均值是总体期望的无偏估计，</p>
<ul>
<li><span class="math inline">\(E(x_i^2) = Var(x_i) + (E(x_i))^2=\sigma^2 + \mu^2\)</span></li>
<li><span class="math inline">\(E(\overline x^2) = Var(\overline x) + (E(\overline x))^2 = Var(\frac{\sum x_i}{n})+\mu^2=\frac{1}{n^2}Var(\sum x_i) + \mu^2 = \frac{n\sigma^2}{n^2} + \mu^2 = \frac{\sigma^2}{n} + \mu^2\)</span></li>
</ul>
<p>所以 <span class="math display">\[
E(s_n^2) = \frac{1}{n}\sum_{i=1}^{n}(\sigma^2+\mu^2) - (\frac{\sigma^2}{n} + \mu^2) = (1-\frac{1}{n})\sigma^2 \le \sigma^2
\]</span></p>
<p><span class="math inline">\(s_n^2\)</span>是总体方差<span class="math inline">\(\sigma^2\)</span>的有偏估计。</p>
<p><span class="math display">\[
s^2 = \frac{Q}{f} = \frac{1}{n-1}\sum_{i=1}{n}(x_i-\overline x)^2
\]</span> <span class="math inline">\(s^2\)</span>是总体方差的无偏估计,自由度是n-1。</p>
<p>证明： <span class="math display">\[
E(s^2) = E(\frac{n}{n-1}s_n^2) = \frac{n}{n-1}(1-\frac{1}{n})\sigma^2 = \sigma^2
\]</span></p>
]]></content>
      <tags>
        <tag>统计量</tag>
        <tag>估计量</tag>
        <tag>样本方差</tag>
      </tags>
  </entry>
  <entry>
    <title>样本的经验分布函数和样本矩</title>
    <url>/2021/02/18/%E5%88%86%E5%B8%83%E5%87%BD%E6%95%B0%E5%92%8C%E6%A0%B7%E6%9C%AC%E7%9F%A9/</url>
    <content><![CDATA[<h1 id="分布函数">分布函数</h1>
<p>设总体X的分布函数是<span class="math inline">\(F(x)\)</span>,从中获得的样本观察值为<span class="math inline">\(x_1,x_2,...x_n\)</span>。 将他们从小到大重新排序，重新编号为<span class="math inline">\(x_(1)\)</span>.</p>
]]></content>
      <tags>
        <tag>经验分布函数</tag>
        <tag>样本矩</tag>
      </tags>
  </entry>
  <entry>
    <title>伽玛函数</title>
    <url>/2021/02/20/%E4%BC%BD%E7%8E%9B%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<h1 id="阶乘">阶乘</h1>
<p>对于任意自然数，<span class="math inline">\(0,1,2,...,n\)</span>的阶乘的定义为 <span class="math inline">\(n! = 1\times 2 \times 3 ... \times n\)</span></p>
<p>但是<span class="math inline">\(0.5!\)</span>怎么计算呢？</p>
<h1 id="伽玛函数">伽玛函数</h1>
<p>1728年，哥德巴赫在考虑数值插值问题时提出的，如何将自然数的阶乘推广到实数集。 将数据点<span class="math inline">\((n,n!)\)</span>画在图形上，好像可以看出大致的趋势，但是无法从数学的角度严 格证明。于是请教同时期的伯努利兄弟。由于欧拉当时还是丹尼尔伯努利的助手。 因此欧拉也得知这个问题，第二年，即1729年，欧拉给出了完美的数据公式， 从此完成了阶乘向实数集的拓展，此时欧拉大神22岁。</p>
<h2 id="年欧拉定义">1730年欧拉定义</h2>
<p><span class="math display">\[
\Gamma(x) = \displaystyle \int^{1}_{0}( -log(t))^{x-1}dt
\]</span></p>
<p>令 <span class="math inline">\(t=e^{-u}\)</span>,由于<span class="math inline">\(t \in (0,1)\)</span>，所以<span class="math inline">\(u \in (0,\infty)\)</span> <span class="math display">\[
\Gamma(x) = \displaystyle \int^{\infty}_{0}u^{x-1}e^{-u}du
\]</span> 令<span class="math inline">\(u=x, x=s\)</span>，则得到了伽玛函数的一般形式。 <span class="math display">\[\begin{equation}
\Gamma(s) = \displaystyle \int^{+\infty}_{0}{x^{s-1}e^{-x}dx}
\end{equation}\]</span></p>
<h2 id="证明过程需要使用分部积分法">证明过程需要使用分部积分法</h2>
<p><span class="math display">\[\begin{equation}
\begin{aligned}

\Gamma(s+1) &amp;= \displaystyle \int^{+\infty}_{0}{x^{s}e^{-x}dx} \\
&amp;= -\displaystyle \int^{\infty}_{0}{x^{s}d(e^{-x})} \\
&amp;= -((x^s e^{-x}|_{0}^{\infty}) - (\displaystyle \int^{\infty}_{0}e^{-x}d(-x^s))) \\
&amp;= -((x^s e^{-x}|_{0}^{\infty}) - \displaystyle \int^{\infty}_{0}s x^{s-1}e^{-x}dx) \\
&amp;= s\Gamma(s)
\end{aligned}
\end{equation}\]</span></p>
<p><span class="math display">\[
\Gamma(1) = \displaystyle \int^{\infty}_{0}e^{-x}dx = 1
\]</span> 规定<span class="math inline">\(0!=1\)</span> 由上面的证明可知，伽玛函数具有递归性质，可以用来进行阶乘的计算。 则<span class="math inline">\(\Gamma(n+1)=n!\)</span>，因此对于任意的数都可以进行阶乘计算。 回到上面的<span class="math inline">\(0.5!\)</span>的计算。</p>
<p><span class="math inline">\(0.5!=\Gamma(0.5+1)=\displaystyle \int^{+\infty}_{0}{x^{0.5}e^{-x}dx}=\frac{1}{2}\sqrt{\pi}\)</span></p>
<h1 id="伽玛分布">伽玛分布</h1>
<p>因为伽玛函数的一般表达式为<span class="math inline">\(\Gamma(\alpha) = \displaystyle \int^{+\infty}_{0}{x^{\alpha-1}e^{-x}dx}\)</span> 等式两边同时除以<span class="math inline">\(\Gamma(\alpha)\)</span>,则得</p>
<p><span class="math inline">\(1=\displaystyle \int^{+\infty}_{0}\frac{x^{\alpha-1}e^{-x}}{\Gamma(\alpha)}d(x)\)</span></p>
<p>令 <span class="math inline">\(x=\lambda x\)</span>，代入上式， 则上式如下： <span class="math display">\[\begin{equation}
\begin{aligned}
1 &amp;= \displaystyle \int^{+\infty}_{0}\frac{x^{\alpha-1}e^{-x}}{\Gamma(\alpha)}d(x) \\
&amp;= \displaystyle \int^{+\infty}_{0}\frac{(\lambda x)^{\alpha-1}e^{-(\lambda x)}}{\Gamma(\alpha)}d(\lambda x) \\
&amp;= \displaystyle \int^{+\infty}_{0}\frac{(\lambda x)^{\alpha-1}e^{-\lambda x}}{\Gamma(\alpha)} \cdot \lambda d(x) \\
&amp;= \displaystyle \int^{+\infty}_{0}\frac{\lambda^\alpha x^{\alpha-1}e^{-\lambda x}}{\Gamma(\alpha)}d(x) \\
\end{aligned}
\end{equation}\]</span></p>
<p>取上式中的的被积函数作为伽马分布的密度函数</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
p(x) &amp;= \frac{\lambda^\alpha x^{\alpha-1}e^{-\lambda x}}{\Gamma(\alpha)} \\
&amp;= \frac{\lambda^\alpha}{\Gamma(\alpha)} x^{\alpha-1}e^{-\lambda x}
\end{aligned}
\end{equation}\]</span></p>
<h2 id="伽玛分布的期望和方差">伽玛分布的期望和方差</h2>
<h3 id="期望的证明">期望的证明</h3>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
E(x) &amp;= \sum p(x_i)\cdot x_i \\
&amp;= \frac{\lambda^\alpha}{\Gamma(\alpha)} \displaystyle \int^{+\infty}_{0} x^{\alpha-1}e^{-\lambda x} d(x)
\end{aligned}
\end{equation}\]</span></p>
<p>因为 <span class="math inline">\(\Gamma(\alpha+1) = \displaystyle \int^{+\infty}_{0} x^{\alpha}e^{-\lambda x}\)</span>,</p>
<p>令<span class="math inline">\(x=\lambda x\)</span>,则<span class="math inline">\(\Gamma(\alpha+1)\)</span> <span class="math display">\[\begin{equation}
\begin{aligned}
\Gamma(\alpha+1) &amp;= \displaystyle \int^{+\infty}_{0} \lambda^\alpha x^{\alpha}e^{-\lambda x} d(\lambda x)d(x) \\
&amp;=\lambda \displaystyle \int^{+\infty}_{0} \lambda^\alpha x^{\alpha}e^{-\lambda x} d(x)
\end{aligned}
\end{equation}\]</span></p>
<p>所以 <span class="math display">\[\begin{equation}
\begin{aligned}
E(x) &amp;= \frac{\frac{\Gamma(\alpha+1)}{\lambda}}{\Gamma(x)} \\
&amp;= \frac{\Gamma(\alpha+1)}{\Gamma(\alpha)}\cdot\frac{1}{\lambda} \\
&amp;= \frac{\alpha}{\lambda}
\end{aligned}
\end{equation}\]</span></p>
<h3 id="方差的证明">方差的证明</h3>
<p>因为<span class="math inline">\(p(x) = \frac{\lambda^\alpha}{\Gamma(\alpha)}x^(\alpha-1)e^{-\lambda x}\)</span>, 所以对于离散型随机变量，<span class="math inline">\(E(x^2) = p(x)\cdot x^2\)</span> 对于连续型随机变量，二阶原点矩如下： <span class="math display">\[\begin{equation}
\begin{aligned}
E(x^2) &amp;= \displaystyle \int^{+\infty}_{0} x^2 \frac{\lambda^\alpha}{\Gamma(\alpha)} x^{\alpha-1}e^{-\lambda x}d(x) \\
&amp;= \displaystyle \int^{+\infty}_{0}\frac{\lambda^\alpha}{\Gamma(\alpha)} x^{\alpha+1}e^{-\lambda x}d(x)
\end{aligned}
\end{equation}\]</span></p>
<p>因为<span class="math inline">\(\Gamma(\alpha+1)=\lambda \displaystyle \int^{+\infty}_{0} \lambda^\alpha x^{\alpha}e^{-\lambda x}d(x)\)</span></p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\Gamma(\alpha+2) &amp;= \lambda \displaystyle \int^{+\infty}_{0} \lambda^{\alpha+1} x^{\alpha+1}e^{-\lambda x}d(x) \\
&amp;= \lambda^2 \displaystyle \int^{+\infty}_{0} \lambda^\alpha x^{\alpha+1}e^{-\lambda x}d(x) 
\end{aligned}
\end{equation}\]</span></p>
<p>由上式可知<span class="math inline">\(\displaystyle \int^{+\infty}_{0} \lambda^\alpha x^{\alpha+1}e^{-\lambda x} d(x) = \frac{\Gamma(\alpha+2)}{\lambda^2}\)</span> <span class="math display">\[\begin{equation}
\begin{aligned}
E(x^2) &amp;= \frac{\displaystyle \int^{+\infty}_{0}\lambda^\alpha x^{\alpha+1}e^{-\lambda x}d(x)}{\Gamma(\alpha)} \\
&amp;= \frac{\Gamma(\alpha+2)}{\lambda^2\Gamma(\alpha)} \\
&amp;= \frac{\alpha(\alpha+1)}{\lambda^2}
\end{aligned}
\end{equation}\]</span></p>
<p>根据方差的性质<span class="math inline">\(Var(x) = E(x^2) - E(x)^2\)</span></p>
<p>所以伽玛分布的方差为： <span class="math display">\[\begin{equation}
\begin{aligned}
Var(x) &amp;= \frac{\alpha(\alpha+1)}{\lambda^2} - {(\frac{\alpha}{\lambda})}^2 \\
&amp;= \frac{\alpha}{\lambda^2}
\end{aligned}
\end{equation}\]</span></p>
]]></content>
      <tags>
        <tag>伽玛函数</tag>
        <tag>阶乘</tag>
      </tags>
  </entry>
  <entry>
    <title>几何分布</title>
    <url>/2021/03/11/%E5%87%A0%E4%BD%95%E5%88%86%E5%B8%83/</url>
    <content><![CDATA[<h1 id="几何分布">几何分布</h1>
<p>时间A发生的概率为p的伯努利实验中，<span class="math inline">\(\xi\)</span>记为事件A首次出现的 实验次数，则称随机变量<span class="math inline">\(\xi\)</span>服从几何分布，</p>
<p><span class="math display">\[
g(k,p) = P(\xi=k) = q^{k-1}p, k= 1,2,...
\]</span></p>
<h1 id="几何分布的无记忆性">几何分布的无记忆性</h1>
<p>在伯努利实验中，等待首次成功的时间<span class="math inline">\(\xi\)</span>服从几何分布。假定前m次都没有成功， 那么首次成功等待的时间为<span class="math inline">\(\xi^{&#39;}\)</span>,也是服从几何分布的。 证明： 根据条件概率的定义 <span class="math display">\[
P = \frac{q^{m+k+1}p}{q^m} = q^{k-1}p
\]</span></p>
]]></content>
      <tags>
        <tag>geometrix distribution</tag>
      </tags>
  </entry>
  <entry>
    <title>巴斯卡分布</title>
    <url>/2021/03/12/%E5%B7%B4%E6%96%AF%E5%8D%A1%E5%88%86%E5%B8%83/</url>
    <content><![CDATA[<h1 id="巴斯卡分布">巴斯卡分布</h1>
<p>伯努利事件中，记<span class="math inline">\(\xi\)</span>为第r次成功出现的实验次数，则<span class="math inline">\(\xi\)</span>是随机变量， 取值为r,r+1,r+2,...,其概率分布为巴斯卡分布 <span class="math display">\[
P(\xi=k) = {k-1 \choose (r-1)}p^rq^{k-r}, k=r,r+1,r+2,...
\]</span> 显然，单r=1时，即为几何分布</p>
<h1 id="负二项分布">负二项分布</h1>
<p>巴斯卡分布规定事件发生的次数为正整数r&gt;=0，当除掉这个限制后，巴斯卡分布就拓展为负二项分布。 现在规定，伯努利事件发生的概率为p，伯努利试验发生的总次数为r+k,事件刚好在r+k次发生第r次的 概率为<span class="math inline">\(f(k;r;p)\)</span></p>
<p><span class="math display">\[
f(k;r;p) = {k+r-1 \choose (r-1)}\cdot p^r \cdot (1-p)^k
\]</span></p>
]]></content>
      <tags>
        <tag>巴斯卡分布</tag>
      </tags>
  </entry>
  <entry>
    <title>sigmoid求导</title>
    <url>/2021/05/08/sigmoid%E6%B1%82%E5%AF%BC/</url>
    <content><![CDATA[<h1 id="sigmoid">sigmoid</h1>
<p>sigmoid 函数是机器学习常用的一种激活函数，公式如下： <span class="math display">\[
f(x) = \frac{1}{1+e^{-x}} 
\]</span></p>
<h1 id="求导">求导</h1>
<p><span class="math display">\[\begin{equation}
\begin{aligned}

f(x) &amp;= ((1+e^{-x})^{-1}) \\
&amp;= (-1)\times(1+e^{-x})^{-2} \times e^{-x} \times (-1) \\
&amp;= (1+e^{-x})^{-2} \times e^{-x} \\
&amp;= \frac{e^{-x}}{(1+e^{-x})^2} \\
&amp;= \frac{1+e^{-x}-1}{(1+e^{-x})^2} \\
&amp;= \frac{1+e^{-x}}{(1+e^{-2})^2} - \frac{1}{(1+e^{-x})^2} \\
&amp;= \frac{1}{1+e^{-x}} - \frac{1}{(1+e^{-x})^2} \\
&amp;= \frac{1}{1+e^{-x}}(1-\frac{1}{1+e^{-x}})

\end{aligned}
\end{equation}\]</span></p>
]]></content>
  </entry>
</search>
